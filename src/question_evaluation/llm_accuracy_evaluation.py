# -*- coding: utf-8 -*-
"""performance_evaluation.ipynb

Automatically generated by Colab.

### Evaluating GPT-3.5
"""

# Import necessary libraries
import json
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files

# Upload your JSON files
uploaded = files.upload()

# Load JSON data
ground_truth = json.load(open('med_calc_qa.json', 'r'))
gpt_results = json.load(open('medcalcqa_gpt-35-turbo.json', 'r'))

# Extract answers and calculator types from ground truth
ground_truth_answers = {key: value['answer'] for key, value in ground_truth.items()}
ground_truth_calculator_types = {key: value['options'][value['answer']] for key, value in ground_truth.items()}

# Extract answers from GPT results
gpt_answers = {key: value['answer'] for key, value in gpt_results.items()}

# Ensure all keys are aligned and handle missing keys
common_keys = set(ground_truth_answers.keys()).intersection(gpt_answers.keys())

# Create a DataFrame for comparison
comparison_df = pd.DataFrame({
    'Question_ID': list(common_keys),
    'Ground_Truth_Answer': [ground_truth_answers[key] for key in common_keys],
    'GPT_Answer': [gpt_answers[key] for key in common_keys],
    'Calculator_Type': [ground_truth_calculator_types[key] for key in common_keys]
})

# Calculate accuracy
comparison_df['Correct'] = comparison_df['Ground_Truth_Answer'] == comparison_df['GPT_Answer']
accuracy = comparison_df['Correct'].mean()

# Print the accuracy
print(f"Accuracy of GPT-35-Turbo: {accuracy * 100:.2f}%")

# Create a palette with shades for correct and incorrect answers
palette = sns.color_palette("Blues", n_colors=2)

# Group by Calculator_Type and Correct, then calculate the count and proportion
grouped_df = comparison_df.groupby(['Calculator_Type', 'Correct']).size().unstack(fill_value=0)
grouped_df = grouped_df.div(grouped_df.sum(axis=1), axis=0)

# Reorder the columns to have True first and then False
grouped_df = grouped_df.reindex(columns=[True, False])

# Plot the horizontal bar plot with stacked bars for correct and incorrect
plt.figure(figsize=(14, 7))
ax = grouped_df.plot(kind='barh', stacked=True, color=[palette[1], palette[0]], figsize=(14, 7))

plt.title('GPT-35-Turbo Answer Accuracy by Calculator Type')
plt.xlabel('Proportion')
plt.ylabel('Calculator Type')
plt.legend(['Correct', 'Incorrect'], bbox_to_anchor=(1.05, 1), loc='upper left')
plt.xlim(0, 1)

plt.show()

"""### Evaluating GPT-4o"""

# Load JSON data
ground_truth = json.load(open('med_calc_qa.json', 'r'))
gpt_results = json.load(open('medcalcqa_gpt-4o.json', 'r'))

# Extract answers and calculator types from ground truth
ground_truth_answers = {key: value['answer'] for key, value in ground_truth.items()}
ground_truth_calculator_types = {key: value['options'][value['answer']] for key, value in ground_truth.items()}

# Extract answers from GPT results
gpt_answers = {key: value['answer'] for key, value in gpt_results.items()}

# Ensure all keys are aligned and handle missing keys
common_keys = set(ground_truth_answers.keys()).intersection(gpt_answers.keys())

# Create a DataFrame for comparison
comparison_df = pd.DataFrame({
    'Question_ID': list(common_keys),
    'Ground_Truth_Answer': [ground_truth_answers[key] for key in common_keys],
    'GPT_Answer': [gpt_answers[key] for key in common_keys],
    'Calculator_Type': [ground_truth_calculator_types[key] for key in common_keys]
})

# Calculate accuracy
comparison_df['Correct'] = comparison_df['Ground_Truth_Answer'] == comparison_df['GPT_Answer']
accuracy = comparison_df['Correct'].mean()

# Print the accuracy
print(f"Accuracy of GPT-4o: {accuracy * 100:.2f}%")

# Create a palette with shades for correct and incorrect answers
palette = sns.color_palette("Blues", n_colors=2)

# Group by Calculator_Type and Correct, then calculate the count and proportion
grouped_df = comparison_df.groupby(['Calculator_Type', 'Correct']).size().unstack(fill_value=0)
grouped_df = grouped_df.div(grouped_df.sum(axis=1), axis=0)

# Reorder the columns to have True first and then False
grouped_df = grouped_df.reindex(columns=[True, False])

# Plot the horizontal bar plot with stacked bars for correct and incorrect
plt.figure(figsize=(14, 7))
ax = grouped_df.plot(kind='barh', stacked=True, color=[palette[1], palette[0]], figsize=(14, 7))

plt.title('GPT-4o Answer Accuracy by Calculator Type')
plt.xlabel('Proportion')
plt.ylabel('Calculator Type')
plt.legend(['Correct', 'Incorrect'], bbox_to_anchor=(1.05, 1), loc='upper left')
plt.xlim(0, 1)

plt.show()

"""### Comparing GPT-3.5 and GPT-4o"""

# Load JSON data
with open('med_calc_qa.json', 'r') as file:
    ground_truth = json.load(file)

with open('medcalcqa_gpt-35-turbo.json', 'r') as file:
    gpt35_results = json.load(file)

with open('medcalcqa_gpt-4o.json', 'r') as file:
    gpt4o_results = json.load(file)

# Extract answers and calculator names from ground truth
ground_truth_answers = {key: value['answer'] for key, value in ground_truth.items()}
ground_truth_calculators = {key: value['options'][value['answer']] for key, value in ground_truth.items()}

# Extract answers from GPT-35 and GPT-4o results
gpt35_answers = {key: value['answer'] for key, value in gpt35_results.items()}
gpt4o_answers = {key: value['answer'] for key, value in gpt4o_results.items()}

# Create a DataFrame for comparison
comparison_df = pd.DataFrame({
    'Question_ID': list(ground_truth_answers.keys()),
    'Ground_Truth_Answer': list(ground_truth_answers.values()),
    'Correct_Calculator': [ground_truth_calculators[key] for key in ground_truth_answers.keys()],
    'GPT35_Answer': [gpt35_answers.get(key, None) for key in ground_truth_answers.keys()],
    'GPT4o_Answer': [gpt4o_answers.get(key, None) for key in ground_truth_answers.keys()]
})

# Determine correctness of each answer
comparison_df['GPT35_Correct'] = comparison_df['Ground_Truth_Answer'] == comparison_df['GPT35_Answer']
comparison_df['GPT4o_Correct'] = comparison_df['Ground_Truth_Answer'] == comparison_df['GPT4o_Answer']

# Find questions where GPT-4o succeeded and GPT-35 failed
gpt4o_success_gpt35_failure = comparison_df[(comparison_df['GPT4o_Correct']) & (~comparison_df['GPT35_Correct'])]

# Display the resulting DataFrame
gpt4o_success_gpt35_failure

print(len(gpt4o_success_gpt35_failure))

"""### Meditron-70B"""

# Load JSON data
ground_truth = json.load(open('med_calc_qa.json', 'r'))
gpt_results = json.load(open('epfl-llm_meditron-70b.json', 'r'))

# Extract answers and calculator types from ground truth
ground_truth_answers = {key: value['answer'] for key, value in ground_truth.items()}
ground_truth_calculator_types = {key: value['options'][value['answer']] for key, value in ground_truth.items()}

# Extract answers from GPT results
gpt_answers = {key: value['answer'] for key, value in gpt_results.items()}

# Ensure all keys are aligned and handle missing keys
common_keys = set(ground_truth_answers.keys()).intersection(gpt_answers.keys())

# Create a DataFrame for comparison
comparison_df = pd.DataFrame({
    'Question_ID': list(common_keys),
    'Ground_Truth_Answer': [ground_truth_answers[key] for key in common_keys],
    'GPT_Answer': [gpt_answers[key] for key in common_keys],
    'Calculator_Type': [ground_truth_calculator_types[key] for key in common_keys]
})

# Calculate accuracy
comparison_df['Correct'] = comparison_df['Ground_Truth_Answer'] == comparison_df['GPT_Answer']
accuracy = comparison_df['Correct'].mean()

# Print the accuracy
print(f"Accuracy of Meditron-70B: {accuracy * 100:.2f}%")

"""### Llama-3-8B


"""

# Load JSON data
ground_truth = json.load(open('med_calc_qa.json', 'r'))
gpt_results = json.load(open('meta-llama_Meta-Llama-3-8B-Instruct.json', 'r'))

# Extract answers and calculator types from ground truth
ground_truth_answers = {key: value['answer'] for key, value in ground_truth.items()}
ground_truth_calculator_types = {key: value['options'][value['answer']] for key, value in ground_truth.items()}

# Extract answers from GPT results
gpt_answers = {key: value['answer'] for key, value in gpt_results.items()}

# Ensure all keys are aligned and handle missing keys
common_keys = set(ground_truth_answers.keys()).intersection(gpt_answers.keys())

# Create a DataFrame for comparison
comparison_df = pd.DataFrame({
    'Question_ID': list(common_keys),
    'Ground_Truth_Answer': [ground_truth_answers[key] for key in common_keys],
    'GPT_Answer': [gpt_answers[key] for key in common_keys],
    'Calculator_Type': [ground_truth_calculator_types[key] for key in common_keys]
})

# Calculate accuracy
comparison_df['Correct'] = comparison_df['Ground_Truth_Answer'] == comparison_df['GPT_Answer']
accuracy = comparison_df['Correct'].mean()

# Print the accuracy
print(f"Accuracy of Llama-3-8B: {accuracy * 100:.2f}%")

"""### Llama-3-70B"""

# Load JSON data
ground_truth = json.load(open('med_calc_qa.json', 'r'))
gpt_results = json.load(open('meta-llama_Meta-Llama-3-70B-Instruct.json', 'r'))

# Extract answers and calculator types from ground truth
ground_truth_answers = {key: value['answer'] for key, value in ground_truth.items()}
ground_truth_calculator_types = {key: value['options'][value['answer']] for key, value in ground_truth.items()}

# Extract answers from GPT results
gpt_answers = {key: value['answer'] for key, value in gpt_results.items()}

# Ensure all keys are aligned and handle missing keys
common_keys = set(ground_truth_answers.keys()).intersection(gpt_answers.keys())

# Create a DataFrame for comparison
comparison_df = pd.DataFrame({
    'Question_ID': list(common_keys),
    'Ground_Truth_Answer': [ground_truth_answers[key] for key in common_keys],
    'GPT_Answer': [gpt_answers[key] for key in common_keys],
    'Calculator_Type': [ground_truth_calculator_types[key] for key in common_keys]
})

# Calculate accuracy
comparison_df['Correct'] = comparison_df['Ground_Truth_Answer'] == comparison_df['GPT_Answer']
accuracy = comparison_df['Correct'].mean()

# Print the accuracy
print(f"Accuracy of Llama-3-70B: {accuracy * 100:.2f}%")

"""### Mistral-7B"""

gpt_results = json.load(open('mistralai_Mistral-7B-Instruct-v0.2.json', 'r'))

# Extract answers and calculator types from ground truth
ground_truth_answers = {key: value['answer'] for key, value in ground_truth.items()}
ground_truth_calculator_types = {key: value['options'][value['answer']] for key, value in ground_truth.items()}

# Extract answers from GPT results
gpt_answers = {key: value['answer'] for key, value in gpt_results.items()}

# Ensure all keys are aligned and handle missing keys
common_keys = set(ground_truth_answers.keys()).intersection(gpt_answers.keys())

# Create a DataFrame for comparison
comparison_df = pd.DataFrame({
    'Question_ID': list(common_keys),
    'Ground_Truth_Answer': [ground_truth_answers[key] for key in common_keys],
    'GPT_Answer': [gpt_answers[key] for key in common_keys],
    'Calculator_Type': [ground_truth_calculator_types[key] for key in common_keys]
})

# Calculate accuracy
comparison_df['Correct'] = comparison_df['Ground_Truth_Answer'] == comparison_df['GPT_Answer']
accuracy = comparison_df['Correct'].mean()

# Print the accuracy
print(f"Accuracy of Mistral-7B: {accuracy * 100:.2f}%")

"""### Mistral"""

gpt_results = json.load(open('mistralai_Mixtral-8x7B-Instruct-v0.1.json', 'r'))

# Extract answers and calculator types from ground truth
ground_truth_answers = {key: value['answer'] for key, value in ground_truth.items()}
ground_truth_calculator_types = {key: value['options'][value['answer']] for key, value in ground_truth.items()}

# Extract answers from GPT results
gpt_answers = {key: value['answer'] for key, value in gpt_results.items()}

# Ensure all keys are aligned and handle missing keys
common_keys = set(ground_truth_answers.keys()).intersection(gpt_answers.keys())

# Create a DataFrame for comparison
comparison_df = pd.DataFrame({
    'Question_ID': list(common_keys),
    'Ground_Truth_Answer': [ground_truth_answers[key] for key in common_keys],
    'GPT_Answer': [gpt_answers[key] for key in common_keys],
    'Calculator_Type': [ground_truth_calculator_types[key] for key in common_keys]
})

# Calculate accuracy
comparison_df['Correct'] = comparison_df['Ground_Truth_Answer'] == comparison_df['GPT_Answer']
accuracy = comparison_df['Correct'].mean()

# Print the accuracy
print(f"Accuracy of Mistral-8x7B: {accuracy * 100:.2f}%")

"""axiong_PMC_LLaMA_13B"""

gpt_results = json.load(open('axiong_PMC_LLaMA_13B.json', 'r'))

# Extract answers and calculator types from ground truth
ground_truth_answers = {key: value['answer'] for key, value in ground_truth.items()}
ground_truth_calculator_types = {key: value['options'][value['answer']] for key, value in ground_truth.items()}

# Extract answers from GPT results
gpt_answers = {key: value['answer'] for key, value in gpt_results.items()}

# Ensure all keys are aligned and handle missing keys
common_keys = set(ground_truth_answers.keys()).intersection(gpt_answers.keys())

# Create a DataFrame for comparison
comparison_df = pd.DataFrame({
    'Question_ID': list(common_keys),
    'Ground_Truth_Answer': [ground_truth_answers[key] for key in common_keys],
    'GPT_Answer': [gpt_answers[key] for key in common_keys],
    'Calculator_Type': [ground_truth_calculator_types[key] for key in common_keys]
})

# Calculate accuracy
comparison_df['Correct'] = comparison_df['Ground_Truth_Answer'] == comparison_df['GPT_Answer']
accuracy = comparison_df['Correct'].mean()

# Print the accuracy
print(f"Accuracy of PMC-LLaMA-13B: {accuracy * 100:.2f}%")